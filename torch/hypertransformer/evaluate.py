"""Online model evaluation script."""

import functools
import os

from typing import Optional, Callable

from absl import app, flags, logging
import numpy as np
import torch

from hypertransformer import common_flags
from hypertransformer import eval_model_flags  # pylint:disable=unused-import
from hypertransformer import train
from hypertransformer.core import common_ht
from hypertransformer.core import evaluation_lib as eval_lib
from hypertransformer.core import layerwise
from hypertransformer.core import layerwise_defs  # pylint:disable=unused-import
from hypertransformer.core import util

FLAGS = flags.FLAGS

DS_PRESETS = {
    "miniimagenet": {
        "train": "miniimagenet:0-63",
        "val": "miniimagenet:64-79",
        "test": "miniimagenet:80-99",
    },
    "omniglot": {
        "train": "omniglot:0-1149",
        "val": "omniglot:1150-1199",
        "test": "omniglot:1200-1622",
    },
}

SECONDS_SLEEP = 10
SUMMARY_SUBFOLDER = "eval"


def make_data_configs():
    """Creates dataset config for all datasets that need to be evaluated."""
    eval_datasets = str(FLAGS.eval_datasets)
    if eval_datasets in DS_PRESETS:
        datasets = DS_PRESETS[eval_datasets]
    else:
        raise ValueError(
            f"Dataset collection {eval_datasets} is not in presets. "
            "Manual dataset specifications are not currently "
            "supported."
        )

    def _builder(name):
        builders = {
            "train": train.make_dataset_config,
            "val": train.make_test_dataset_config,
            "test": train.make_test_dataset_config,
        }
        return builders[name]

    return {name: _builder(name)(dataset) for name, dataset in datasets.items()}


def run_evaluation(
    dataset_info: dict[
        str,
        tuple[
            Callable[[], "common_ht.DatasetSamples"],
            Optional[list[int]],
        ],
    ],
    eval_config: eval_lib.EvaluationConfig,
    model: layerwise.LayerwiseModel,
):
    """Runs model evaluation loop over a set of datasets.

       1) Transformer looks at the support images + labels. 
       2) CNN uses the weights generated by the Transformer to classify the query images.

         Trainer:
           step 1000 → save ckpt
           step 2000 → save ckpt
           step 3000 → save ckpt
                ↓
         Evaluator:
           detect ckpt-1000 → evaluate
           detect ckpt-2000 → evaluate
           detect ckpt-3000 → evaluate
    """
    ss_dir = os.path.join(FLAGS.train_log_dir, SUMMARY_SUBFOLDER)
    os.makedirs(ss_dir, exist_ok=True)
    summary_writer = util.MultiFileWriter(ss_dir)

    global_step = 0
    last_checkpoint = None

    for name, param in model.named_parameters():
        logging.info(f"{name}, device: {param.device}, grad: {param.grad is None}")

    while True:
        # Continuously monitor the training directory. 
        # Once there is a new checkpoint, load it and conduct an evaluation.
        # This is an "online evaluation" mechanism
        checkpoint = eval_lib.wait_for_new_checkpoint(
            FLAGS.train_log_dir,
            last_checkpoint,
            sleep_seconds=SECONDS_SLEEP,
        )
        last_checkpoint = checkpoint
        logging.info(f"Evaluating checkpoint: {checkpoint}...")

        # Here, the parameters of the Transformer are loaded, not those of a certain CNN. 
        #  Transformer => Weight Generator
        #  CNN         => Temporarily generated model (without persistent parameters)
        ckpt = torch.load(checkpoint, map_location="cpu")
        global_step = int(ckpt.get("global_step", 0))
        model.load_state_dict(ckpt["model"], strict=False)
        model.eval()

        add_scalar = functools.partial(
            summary_writer.add_scalar,
            global_step=global_step,
            writer_name=SUMMARY_SUBFOLDER,
        )
        for name, info in dataset_info.items():
            batch_provider, custom_labels = info
            _, accs = eval_lib.evaluate_dataset(
                custom_labels=custom_labels,
                batch_provider=batch_provider,
                model=model,
                eval_config=eval_config,
            )
            add_scalar(f"eval_accuracy/{name}", float(np.mean(accs)))

        if global_step >= FLAGS.train_steps:
            break
        global_step += 1


def evaluate_pretrained(model_config: common_ht.LayerwiseModelConfig):
    """Evaluates a pretrained 'layerwise' model.

       1) The meta training process continuously saves checkpoints
            ↓
       2) The evaluation process continuously monitors the checkpoints directory
            ↓
       3) Once a new checkpoint is discovered, load the Transformer weights
            ↓
       4) Generate the CNN weights with the Transformer
            ↓
       5) Evaluate the accuracy on few-shot tasks
            ↓
       6) Write to TensorBoard summary
    """
    dataset_configs = make_data_configs()

    is_metadataset = False
    # Meta-dataset batches are provided by the library and we can only get one
    # batch per episode (hence 1 evaluation batch).
    eval_config = eval_lib.EvaluationConfig(
        num_task_evals=FLAGS.num_task_evals,
        num_eval_batches=FLAGS.num_eval_batches if not is_metadataset else 1,
    )
    dataset_info = {
        name: eval_lib.dataset_with_custom_labels(model_config, config)
        for name, config in dataset_configs.items()
    }

    # NOTE: Initialize the layerwise model!!!
    model: Optional[layerwise.LayerwiseModel] = None
    for _, info in dataset_info.items():
        batch_provider, _ = info
        dataset = batch_provider()
        model = layerwise.build_model(
            model_config.cnn_model_name,
            model_config=model_config,
            dataset=dataset,
        )
        with torch.no_grad():
            model.setup(
                dataset.transformer_images,
                dataset.transformer_labels,
                mask=dataset.transformer_masks,
                mask_random_samples=True,
                enable_fe_dropout=True,
                only_shared_feature=False,
            )
        break
    assert model is not None, "Must initialize the layerwise model before evaluation!"

    run_evaluation(
        eval_config=eval_config,
        dataset_info=dataset_info,
        model=model,
    )


def main(argv):
    if len(argv) > 1:
        del argv

    logging.info(f"\nFLAGS: {FLAGS.flag_values_dict()}\n")

    # No need to do evaluation if all we do is pretrain the shared feature.
    if common_flags.PRETRAIN_SHARED_FEATURE.value:
        return

    gpus: str = FLAGS.gpus
    if gpus is None or gpus == "all":
        os.environ.pop("CUDA_VISIBLE_DEVICES", None)
    else:
        os.environ["CUDA_VISIBLE_DEVICES"] = gpus
    util.print_gpu_detailed_info()

    FLAGS.samples_cnn = FLAGS.eval_batch_size
    evaluate_pretrained(train.make_layerwise_model_config())


if __name__ == "__main__":
    app.run(main)
