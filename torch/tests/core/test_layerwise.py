"""Tests for `layerwise.py`."""

import pytest
import torch
import numpy as np

from hypertransformer.core import common_ht
from hypertransformer.core import datasets
from hypertransformer.core import util
from hypertransformer.core import layerwise
from hypertransformer.core import layerwise_defs # pylint:disable=unused-import
from hypertransformer.core import train_lib


@pytest.fixture(params=[
    {
        "num_transformer_samples": 8,
        "num_cnn_samples": 8,
        "num_samples_per_label": 8,
        "num_labels": 4,
        "channels": 1,
        "image_size": 32,
    },
    {
        "num_transformer_samples": 2,
        "num_cnn_samples": 6,
        "num_samples_per_label": 10,
        "num_labels": 5,
        "channels": 3,
        "image_size": 64,
    },
])
def layerwise_params(request):
    p = request.param
    num_transformer_samples = p["num_transformer_samples"]
    num_cnn_samples = p["num_cnn_samples"]
    num_samples_per_label = p["num_samples_per_label"]
    num_labels = p["num_labels"]
    channels = p["channels"]
    image_size = p["image_size"]

    torch.manual_seed(0)

    batch_size = num_samples_per_label * num_labels
    assert batch_size % num_labels == 0
    repetitions = batch_size // num_labels

    # (batch_size, channels, image_size, image_size)
    images = np.zeros(
        (batch_size, channels, image_size, image_size),
        dtype=np.float32,
    )
    # [0,1,..,num_labels-1, ..., 0,1,..,num_labels-1]
    labels = np.array(list(range(num_labels))*repetitions, dtype=np.int32)
    ds = datasets.HTDataset(images, labels)

    model_config = common_ht.LayerwiseModelConfig(
        num_transformer_samples=num_transformer_samples,
        num_cnn_samples=num_cnn_samples,
        image_size=image_size,
    )
    model_config.shared_input_dim = channels

    dataset_info = common_ht.DatasetInfo(
        num_labels=num_labels,
        num_samples_per_label=num_samples_per_label,
        transpose_images=False,
    )

    dataset, label_set = util.parse_dataset_spec(f"emnist:0-{num_labels-1}")
    data_config = common_ht.DatasetConfig(
        dataset_name="dataset",
        ds=ds,
        dataset_info=dataset_info,
        use_label_subset=label_set,
    )

    numpy_arr = train_lib.make_numpy_array(
        data_config,
        batch_size=model_config.num_transformer_samples,
    )
    dataset = train_lib.make_dataset(
        numpy_arr=numpy_arr,
        model_config=model_config,
        data_config=data_config,
        shuffle_labels=True,
    )

    return (model_config, dataset)

def test_number_of_trained_cnn_layers_param_should_give_trained_weights(layerwise_params):
    """Tests the layerwise model with both generated and trained weights."""
    (
        model_config,
        dataset,
    ) = layerwise_params
    model_config.number_of_trained_cnn_layers = 1
    model_config.shared_feature_extractor = "4-layer"

    model = layerwise.build_model(
        name=model_config.cnn_model_name,
        model_config=model_config,
        dataset=dataset,
    )
    weights = model._train(dataset.transformer_images, dataset.transformer_labels)

    # The first CNN layer is directly trained → no weights are generated
    assert weights.weight_blocks[0] is None
    # The subsequent CNN layer is generated by HyperTransformer
    for weight_block in weights.weight_blocks[1:]:
        assert weight_block is not None

    model._evaluate(dataset.cnn_images, weight_blocks=weights)

    # import pdb
    # pdb.set_trace()
    # The first layer: real trainable parameters
    first_kernel = model.layers[0].kernel
    assert isinstance(first_kernel, torch.nn.Parameter)
    assert first_kernel.requires_grad is True

    # The second layer: `Tensor` generated by the Transformer
    second_kernel = model.layers[1].kernel
    assert isinstance(second_kernel, torch.Tensor)
    # torch.nn.Parameter ⊂ torch.Tensor
    assert not isinstance(second_kernel, torch.nn.Parameter)

def test_negative_number_of_trained_cnn_layers_param_trains_last_layers(layerwise_params):
    """Tests the layerwise model with both generated and trained weights."""
    (
        model_config,
        dataset,
    ) = layerwise_params
    model_config.number_of_trained_cnn_layers = -1

    model = layerwise.build_model(
        model_config.cnn_model_name,
        model_config=model_config,
        dataset=dataset,
    )

    weights = model._train(dataset.transformer_images, dataset.transformer_labels)

    # The second-to-last layer is directly trained
    assert weights.weight_blocks[-2] is None
    # All the previous layers were generated
    for weight_block in weights.weight_blocks[:-2]:
        assert weight_block is not None

    model._evaluate(dataset.cnn_images, weight_blocks=weights)

    # The penultimate layer: real trainable parameters
    last_trained_kernel = model.layers[-2].kernel
    assert isinstance(last_trained_kernel, torch.nn.Parameter)
    assert last_trained_kernel.requires_grad is True

    # Layer 1: `Tensor` generated by the Transformer
    first_kernel = model.layers[0].kernel
    assert isinstance(first_kernel, torch.Tensor)
    assert not isinstance(first_kernel, torch.nn.Parameter)

def test_layer_with_activation_after_bn_different_activation_before_bn(layerwise_params):
    """Tests the option to use activation before or after batchnorm."""
    (
        model_config,
        dataset,
    ) = layerwise_params

    def act_fn(x):
        return torch.ones_like(x)

    layer_act_after = layerwise.ConvLayer(
        name="test_layer_activation_after",
        model_config=model_config,
        act_fn=act_fn,
        act_after_bn=True,
    )
    layer_act_after._setup(dataset.cnn_images)
    layer_act_before = layerwise.ConvLayer(
        name="test_layer_activation_before",
        model_config=model_config,
        act_fn=act_fn,
        act_after_bn=False,
    )
    layer_act_before._setup(dataset.cnn_images)

    out_after = layer_act_after(dataset.cnn_images)
    out_before = layer_act_before(dataset.cnn_images)

    # act_after_bn=True → The final step is `act_fn` → all 1s
    assert torch.allclose(
        out_after,
        torch.ones_like(out_after),
    ), (
        "Activation after BatchNorm not computed correctly."
    )

    # act_after_bn=False → `act_fn` before BN → BN output mean 0
    assert torch.allclose(
        out_before,
        torch.zeros_like(out_before),
        atol=1e-6,
    ), (
        "Activation before BatchNorm not computed correctly."
    )
