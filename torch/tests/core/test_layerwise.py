"""Tests for `layerwise.py`."""

import torch

from hypertransformer.core import common_ht
from hypertransformer.core import layerwise
from hypertransformer.core import layerwise_defs # pylint:disable=unused-import


def make_layerwise_model_config():
    """Makes 'layerwise' model config."""
    return common_ht.LayerwiseModelConfig()

def test_number_of_trained_cnn_layers_param_should_give_trained_weights():
    """Tests the layerwise model with both generated and trained weights."""
    torch.manual_seed(0)

    model_config = make_layerwise_model_config()
    model_config.number_of_trained_cnn_layers = 1
    model = layerwise.build_model(
        model_config.cnn_model_name,
        model_config=model_config,
    )

    images = torch.randn(100, 1, 64, 64)
    labels = torch.randint(0, model_config.num_labels, (100,))
    weights = model._train(images, labels)

    # The first CNN layer is directly trained → no weights are generated
    assert weights.weight_blocks[0] is None
    # The subsequent CNN layer is generated by HyperTransformer
    for weight_block in weights.weight_blocks[1:]:
        assert weight_block is not None

    model._evaluate(images, weight_blocks=weights)

    # The first layer: real trainable parameters
    first_kernel = model.layers[0].kernel
    assert isinstance(first_kernel, torch.nn.Parameter)
    assert first_kernel.requires_grad is True

    # The second layer: `Tensor` generated by the Transformer
    second_kernel = model.layers[1].kernel
    assert isinstance(second_kernel, torch.Tensor)
    # torch.nn.Parameter ⊂ torch.Tensor
    assert not isinstance(second_kernel, torch.nn.Parameter)

def test_negative_number_of_trained_cnn_layers_param_trains_last_layers():
    """Tests the layerwise model with both generated and trained weights."""
    torch.manual_seed(0)

    model_config = make_layerwise_model_config()
    model_config.number_of_trained_cnn_layers = -1
    model = layerwise.build_model(
        model_config.cnn_model_name,
        model_config=model_config,
    )

    images = torch.randn(100, 1, 64, 64)
    labels = torch.randint(0, model_config.num_labels, (100,))

    weights = model._train(images, labels)

    # The second-to-last layer is directly trained
    assert weights.weight_blocks[-2] is None
    # All the previous layers were generated
    for weight_block in weights.weight_blocks[:-2]:
        assert weight_block is not None

    model._evaluate(images, weight_blocks=weights)

    # The penultimate layer: real trainable parameters
    last_trained_kernel = model.layers[-2].kernel
    assert isinstance(last_trained_kernel, torch.nn.Parameter)
    assert last_trained_kernel.requires_grad is True

    # Layer 1: `Tensor` generated by the Transformer
    first_kernel = model.layers[0].kernel
    assert isinstance(first_kernel, torch.Tensor)
    assert not isinstance(first_kernel, torch.nn.Parameter)

def test_layer_with_activation_after_bn_different_activation_before_bn():
    """Tests the option to use activation before or after batchnorm."""
    torch.manual_seed(0)

    model_config = make_layerwise_model_config()
    
    def act_fn(x):
        return torch.ones_like(x)

    layer_act_after = layerwise.ConvLayer(
        name="test_layer_activation_after",
        model_config=model_config,
        act_fn=act_fn,
        act_after_bn=True,
    )
    layer_act_before = layerwise.ConvLayer(
        name="test_layer_activation_before",
        model_config=model_config,
        act_fn=act_fn,
        act_after_bn=False,
    )

    images = torch.randn(100, 3, 64, 64)
    out_after = layer_act_after(images)
    out_before = layer_act_before(images)

    # act_after_bn=True → The final step is `act_fn` → all 1s
    assert torch.allclose(
        out_after,
        torch.ones_like(out_after),
    ), (
        "Activation after BatchNorm not computed correctly."
    )

    # act_after_bn=False → `act_fn` before BN → BN output mean 0
    assert torch.allclose(
        out_before,
        torch.zeros_like(out_before),
        atol=1e-6,
    ), (
        "Activation before BatchNorm not computed correctly."
    )
